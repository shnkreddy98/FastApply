{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/reddy/Documents/GitHub/ResumeBuilder/workingfiles'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<docx.document.Document object at 0x1045b74d0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "resume = Document(SOURCEFILE)\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "SOURCEFILE = \"../ResumeBuilder/inputs/template.docx\"\n",
    "docx_file = SOURCEFILE\n",
    "details = extract_details(docx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Shashank Shashishekhar Reddy', 'location': 'San Jose, California', 'email': 'shashankshashishekharreddy@gmail.com', 'phone': '(510) 892-7191', 'summary': 'Data Engineer with hands-on experience with designing, developing and maintaining Extract, Transform and Load (ETL) pipelines coming from diverse data sources, and creating dashboards tailored to end-user or team requirements, facilitating data-driven decision-making, and ensuring proficiency in version control and data lifecycle management.', 'work_experience': [{'company': 'Data Engineer', 'role': 'Kantar', 'start_date': None, 'end_date': None, 'description': 'Built data pipelines with Apache Airflow and Spark to collect ads and sales data, storing it in an Amazon S3 data lake. Used Apache Spark for data transformation and enrichment, ensuring reliability with schema validation and error handling. Implemented data quality checks to maintain integrity throughout ingestion, transformation, and storage phases. Performed EDA and created visualizations in Tableau and Excel to identify patterns, boosting holiday sales by 12%. Designed ROI models to increase sales by 34% by optimizing ad spend on high-performing channels. Mentored two junior team members, improving team productivity and meeting deadlines with positive stakeholder feedback. '}, {'company': 'Data Engineer', 'role': 'The Sparks Foundation', 'start_date': None, 'end_date': None, 'description': 'Engineered robust ETL pipelines to manage and process large volumes of data from multiple sources, enhancing data integration by 25%. Collaborated with cross-functional teams to optimize database performance, reducing data retrieval times by 30%. Designed and implemented data quality checks to ensure accuracy and reliability, significantly minimizing data discrepancies. Automated data transformation processes using Python and SQL, decreasing manual intervention by 40%. Developed dashboards and visualizations to track data metrics, providing real-time insights for stakeholders. '}, {'company': 'Data Engineer Intern', 'role': 'The Sparks Foundation', 'start_date': None, 'end_date': None, 'description': 'Supported the development of ETL processes, contributing to the seamless integration of new data sources. Conducted data analysis to identify patterns and trends, informing decision-making across departments. Assisted in optimizing SQL queries to improve data extraction efficiency, achieving a 20% reduction in processing time. Created scripts for automated data cleaning, enhancing data quality and accuracy. '}], 'skills': 'Certificates: AWS Cloud Practitioner | Validation Code: QD8E074C82EQ1WCE Programming Languages: C, C++, Java, Python, R, SQL, Bash, JavaScript Cloud Services: AWS (EC2, S3, Lambda, Glue, Amazon S3, Amazon Redshift), GCP, Azure Databases: MySQL, MongoDB, PostgreSQL, RDS, Elasticsearch. Tools: Hadoop, Sqoop, Hive, PySpark, Flume, Airflow, Kafka, Pandas, Scikit-Learn, Keras, PyTorch, Docker, Terraform.', 'projects': [{'project_name': 'Your Own Cabs Analysis', 'start_date': None, 'end_date': None, 'description': 'Built data pipelines to analyze booking and clickstream data using Kafka, Spark, Hadoop, and Hive, driving actionable insights. Ingested clickstream data into Hadoop HDFS via Kafka and batch booking data into AWS RDS with Sqoop, increasing data throughput by 15%. Aggregated data using Spark and stored results in Hive tables to generate customer metrics like ride counts, conversion rates, and service KPIs. '}, {'project_name': 'Spar Nord Bank ETL', 'start_date': None, 'end_date': None, 'description': 'Managed ETL for Spar Nord Bank’s transactional data from MySQL RDS to Amazon Redshift with Apache Sqoop, PySpark, S3, and Redshift. Conducted analysis on Redshift to identify usage patterns, optimizing ATM refilling and reducing refills by 20%. '}], 'education': [{'institution': 'Master of Science in Data Analytics', 'start_date': None, 'end_date': None}, {'institution': 'PG Diploma in Data Science Specialization in Data Engineering', 'start_date': None, 'end_date': None}, {'institution': 'Bachelor’s in Computer Science', 'start_date': None, 'end_date': None}]}\n"
     ]
    }
   ],
   "source": [
    "print(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shashank Shashishekhar Reddy\n",
      "San Jose, California\n",
      "shashankshashishekharreddy@gmail.com\n",
      "(510) 892-7191\n",
      "Data Engineer with hands-on experience with designing, developing and maintaining Extract, Transform and Load (ETL) pipelines coming from diverse data sources, and creating dashboards tailored to end-user or team requirements, facilitating data-driven decision-making, and ensuring proficiency in version control and data lifecycle management.\n",
      "[{'company': 'Data Engineer', 'role': 'Kantar', 'start_date': None, 'end_date': None, 'description': 'Built data pipelines with Apache Airflow and Spark to collect ads and sales data, storing it in an Amazon S3 data lake. Used Apache Spark for data transformation and enrichment, ensuring reliability with schema validation and error handling. Implemented data quality checks to maintain integrity throughout ingestion, transformation, and storage phases. Performed EDA and created visualizations in Tableau and Excel to identify patterns, boosting holiday sales by 12%. Designed ROI models to increase sales by 34% by optimizing ad spend on high-performing channels. Mentored two junior team members, improving team productivity and meeting deadlines with positive stakeholder feedback. '}, {'company': 'Data Engineer', 'role': 'The Sparks Foundation', 'start_date': None, 'end_date': None, 'description': 'Engineered robust ETL pipelines to manage and process large volumes of data from multiple sources, enhancing data integration by 25%. Collaborated with cross-functional teams to optimize database performance, reducing data retrieval times by 30%. Designed and implemented data quality checks to ensure accuracy and reliability, significantly minimizing data discrepancies. Automated data transformation processes using Python and SQL, decreasing manual intervention by 40%. Developed dashboards and visualizations to track data metrics, providing real-time insights for stakeholders. '}, {'company': 'Data Engineer Intern', 'role': 'The Sparks Foundation', 'start_date': None, 'end_date': None, 'description': 'Supported the development of ETL processes, contributing to the seamless integration of new data sources. Conducted data analysis to identify patterns and trends, informing decision-making across departments. Assisted in optimizing SQL queries to improve data extraction efficiency, achieving a 20% reduction in processing time. Created scripts for automated data cleaning, enhancing data quality and accuracy. '}]\n",
      "Certificates: AWS Cloud Practitioner | Validation Code: QD8E074C82EQ1WCE Programming Languages: C, C++, Java, Python, R, SQL, Bash, JavaScript Cloud Services: AWS (EC2, S3, Lambda, Glue, Amazon S3, Amazon Redshift), GCP, Azure Databases: MySQL, MongoDB, PostgreSQL, RDS, Elasticsearch. Tools: Hadoop, Sqoop, Hive, PySpark, Flume, Airflow, Kafka, Pandas, Scikit-Learn, Keras, PyTorch, Docker, Terraform.\n",
      "[{'project_name': 'Your Own Cabs Analysis', 'start_date': None, 'end_date': None, 'description': 'Built data pipelines to analyze booking and clickstream data using Kafka, Spark, Hadoop, and Hive, driving actionable insights. Ingested clickstream data into Hadoop HDFS via Kafka and batch booking data into AWS RDS with Sqoop, increasing data throughput by 15%. Aggregated data using Spark and stored results in Hive tables to generate customer metrics like ride counts, conversion rates, and service KPIs. '}, {'project_name': 'Spar Nord Bank ETL', 'start_date': None, 'end_date': None, 'description': 'Managed ETL for Spar Nord Bank’s transactional data from MySQL RDS to Amazon Redshift with Apache Sqoop, PySpark, S3, and Redshift. Conducted analysis on Redshift to identify usage patterns, optimizing ATM refilling and reducing refills by 20%. '}]\n",
      "[{'institution': 'Master of Science in Data Analytics', 'start_date': None, 'end_date': None}, {'institution': 'PG Diploma in Data Science Specialization in Data Engineering', 'start_date': None, 'end_date': None}, {'institution': 'Bachelor’s in Computer Science', 'start_date': None, 'end_date': None}]\n"
     ]
    }
   ],
   "source": [
    "for key in details.keys():\n",
    "    print(details[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details['education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'location', 'email', 'phone', 'summary', 'work_experience', 'skills', 'projects', 'education'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/reddy/Documents/GitHub/ResumeBuilder/workingfiles/database/mydb1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.abspath('..//mydb1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect(\"/Users/reddy/Documents/GitHub/ResumeBuilder/ResumeBuilder/dbs/ResumeBuilder.db\")\n",
    "con.execute('PRAGMA foreign_keys = ON;')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x107b0e1c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS user (\n",
    "                email_id VARCHAR(255) PRIMARY KEY,\n",
    "                name VARCHAR(255),\n",
    "                user_location VARCHAR(255),\n",
    "                linkedin_link VARCHAR(255),\n",
    "                phone VARCHAR(20),\n",
    "                summary TEXT,\n",
    "                skills TEXT);\n",
    "            \"\"\")\n",
    "\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS work (\n",
    "                work_exp_id INTEGER PRIMARY KEY,\n",
    "                company VARCHAR(255),\n",
    "                role VARCHAR(255),\n",
    "                work_location VARCHAR(255),\n",
    "                work_start_date DATE,\n",
    "                work_end_date DATE,\n",
    "                work_description TEXT,\n",
    "                user_id VARCHAR(255),\n",
    "                FOREIGN KEY(user_id) REFERENCES user(email_id) ON DELETE CASCADE);\n",
    "            \"\"\")\n",
    "\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS project (\n",
    "                project_id INTEGER PRIMARY KEY,\n",
    "                project_name VARCHAR(255),\n",
    "                project_link VARCHAR(255),\n",
    "                project_start_date DATE,\n",
    "                project_end_date DATE,\n",
    "                project_description TEXT,\n",
    "                user_id VARCHAR(255),\n",
    "                FOREIGN KEY(user_id) REFERENCES user(email_id) ON DELETE CASCADE);\n",
    "            \"\"\")\n",
    "\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS education (\n",
    "                education_id INTEGER PRIMARY KEY,\n",
    "                institution_name VARCHAR(255),\n",
    "                course VARCHAR(255),\n",
    "                education_start_date DATE,\n",
    "                education_end_date DATE,\n",
    "                user_id VARCHAR(255),\n",
    "                FOREIGN KEY(user_id) REFERENCES user(email_id) ON DELETE CASCADE);\n",
    "            \"\"\")\n",
    "\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS jobs (\n",
    "                job_id INTEGER PRIMARY KEY,\n",
    "                job_link VARCHAR(255),\n",
    "                job_description TEXT,\n",
    "                hiring_manager_fn VARCHAR(255),\n",
    "                hiring_manager_ln VARCHAR(255),\n",
    "                generated_questions TEXT,\n",
    "                chatgpt_checkpoint TEXT,\n",
    "                user_id VARCHAR(255),\n",
    "                FOREIGN KEY(user_id) REFERENCES user(email_id) ON DELETE CASCADE);\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user',),\n",
       " ('sqlite_autoindex_user_1',),\n",
       " ('work',),\n",
       " ('project',),\n",
       " ('education',),\n",
       " ('jobs',)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = cur.execute(\"SELECT name FROM sqlite_master\")\n",
    "res.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_email_id, user_name, user_city = '', '', ''\n",
    "user_linkedin_link, user_phone,  = '', ''\n",
    "summary, skills = '', ''\n",
    "\n",
    "cur.execute(f\"\"\"INSERT INTO \n",
    "                user (email_id, name, user_location, \n",
    "                        linkedin_link, phone, summary, skills)\n",
    "                    VALUES ({user_email_id}, {user_name}, '{user_city}', \n",
    "                            {user_linkedin_link}, {user_phone}, \n",
    "                            {summary}, {skills});\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ResumeBuilder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
