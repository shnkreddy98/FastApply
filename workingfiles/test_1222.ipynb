{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dict = { \"user_name\": \"Shashank Shashishekhar Reddy\", \"user_email\": \"shashankshashishekharreddy@gmail.com\", \"user_phone\": \"(510) 892-7191\", \"user_city\": \"San Jose, California\", \"user_linkedin_link\": \"https://www.linkedin.com/in/snkreddy\", \"user_summary\": \"Results-driven Data Engineer with extensive experience in administering and enhancing data infrastructure, developing ETL pipelines, and supporting statistical data analysis for manufacturing environments. Skilled in creating efficient data models and facilitating communication across engineering teams. Proficient in Python, SQL, and data visualization tools to deliver actionable insights that improve production performance and drive decision-making.\", \"user_skills\": \"Certificates: AWS Cloud Practitioner | Validation Code: QD8E074C82EQ1WCE Programming Languages: Python, C++, Java, R, SQL, Bash, JavaScript Cloud Services: AWS (EC2, S3, Lambda, Glue, Amazon Redshift), GCP, Azure Databases: MySQL, MongoDB, PostgreSQL, RDS, Elasticsearch. Tools: Hadoop, Spark, Airflow, Tableau, Pandas, Scikit-Learn, Keras, Docker, Terraform. Statistical Analysis: JMP, Origin.\", \"experiences\": [ { \"exp_company\": \"Data Engineer\", \"exp_role\": \"Kantar\", \"exp_start_date\": \"2024-12-03\", \"exp_end_date\": \"2024-12-04\", \"exp_description\": \"Administered and optimized existing data applications to improve production statistics including throughput and yield. Developed ETL pipelines using Apache Airflow and Spark, supporting data analysis for customer-oriented inquiries. Documented application functionality and provided training for end-users, facilitating a smooth user experience.\" }, { \"exp_company\": \"Data Engineer\", \"exp_role\": \"The Sparks Foundation\", \"exp_start_date\": \"2024-12-11\", \"exp_end_date\": \"2024-12-12\", \"exp_description\": \"Engineered robust ETL processes to manage and process vast volumes of data from multiple sources, significantly enhancing data integration. Collaborated with engineering teams to support data inquiries related to product specifications and production processes, reducing data retrieval times by 30%.\" }, { \"exp_company\": \"Data Engineer Intern\", \"exp_role\": \"The Sparks Foundation\", \"exp_start_date\": \"2024-12-04\", \"exp_end_date\": \"2024-12-12\", \"exp_description\": \"Supported the development of data pipelines and conducted thorough analysis to identify trends that inform decision-making. Assisted in optimizing SQL queries and automating data cleaning processes to ensure high data quality and reliability.\" } ], \"projects\": [ { \"proj_name\": \"Your Own Cabs Analysis\", \"proj_start_date\": \"2024-12-04\", \"proj_end_date\": \"2024-12-05\", \"proj_description\": \"Developed data pipelines leveraging Kafka, Spark, and Hadoop to analyze booking and clickstream data. Implemented data models that improved data throughput by 15% and provided critical insights for decision-making, enhancing operational efficiency.\" }, { \"proj_name\": \"Spar Nord Bank ETL\", \"proj_start_date\": \"2024-12-04\", \"proj_end_date\": \"2024-12-12\", \"proj_description\": \"Managed the ETL process for transactional data from MySQL RDS to Amazon Redshift. Conducted thorough data analysis to identify patterns in usage, optimizing ATM refueling processes and achieving a 20% reduction in unnecessary refills.\" } ], \"education\": [ { \"edu_course\": \"Master of Science in Data Analytics\", \"edu_institution\": \"San Jose State University\", \"edu_start_date\": \"2024-12-02\", \"edu_end_date\": \"2024-12-11\" }, { \"edu_course\": \"PG Diploma in Data Science Specialization in Data Engineering\", \"edu_institution\": \"Visvesvaraya Technological University\", \"edu_start_date\": \"2024-12-03\", \"edu_end_date\": \"2024-12-04\" }, { \"edu_course\": \"Bachelor’s in Computer Science\", \"edu_institution\": \"Indian Institute of Information Technology\", \"edu_start_date\": \"2024-12-11\", \"edu_end_date\": \"2024-12-12\" } ] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Shashank Shashishekhar Reddy',\n",
       " 'user_email': 'shashankshashishekharreddy@gmail.com',\n",
       " 'user_phone': '(510) 892-7191',\n",
       " 'user_city': 'San Jose, California',\n",
       " 'user_linkedin_link': 'https://www.linkedin.com/in/snkreddy',\n",
       " 'user_summary': 'Results-driven Data Engineer with extensive experience in administering and enhancing data infrastructure, developing ETL pipelines, and supporting statistical data analysis for manufacturing environments. Skilled in creating efficient data models and facilitating communication across engineering teams. Proficient in Python, SQL, and data visualization tools to deliver actionable insights that improve production performance and drive decision-making.',\n",
       " 'user_skills': 'Certificates: AWS Cloud Practitioner | Validation Code: QD8E074C82EQ1WCE Programming Languages: Python, C++, Java, R, SQL, Bash, JavaScript Cloud Services: AWS (EC2, S3, Lambda, Glue, Amazon Redshift), GCP, Azure Databases: MySQL, MongoDB, PostgreSQL, RDS, Elasticsearch. Tools: Hadoop, Spark, Airflow, Tableau, Pandas, Scikit-Learn, Keras, Docker, Terraform. Statistical Analysis: JMP, Origin.',\n",
       " 'experiences': [{'exp_company': 'Data Engineer',\n",
       "   'exp_role': 'Kantar',\n",
       "   'exp_start_date': '2024-12-03',\n",
       "   'exp_end_date': '2024-12-04',\n",
       "   'exp_description': 'Administered and optimized existing data applications to improve production statistics including throughput and yield. Developed ETL pipelines using Apache Airflow and Spark, supporting data analysis for customer-oriented inquiries. Documented application functionality and provided training for end-users, facilitating a smooth user experience.'},\n",
       "  {'exp_company': 'Data Engineer',\n",
       "   'exp_role': 'The Sparks Foundation',\n",
       "   'exp_start_date': '2024-12-11',\n",
       "   'exp_end_date': '2024-12-12',\n",
       "   'exp_description': 'Engineered robust ETL processes to manage and process vast volumes of data from multiple sources, significantly enhancing data integration. Collaborated with engineering teams to support data inquiries related to product specifications and production processes, reducing data retrieval times by 30%.'},\n",
       "  {'exp_company': 'Data Engineer Intern',\n",
       "   'exp_role': 'The Sparks Foundation',\n",
       "   'exp_start_date': '2024-12-04',\n",
       "   'exp_end_date': '2024-12-12',\n",
       "   'exp_description': 'Supported the development of data pipelines and conducted thorough analysis to identify trends that inform decision-making. Assisted in optimizing SQL queries and automating data cleaning processes to ensure high data quality and reliability.'}],\n",
       " 'projects': [{'proj_name': 'Your Own Cabs Analysis',\n",
       "   'proj_start_date': '2024-12-04',\n",
       "   'proj_end_date': '2024-12-05',\n",
       "   'proj_description': 'Developed data pipelines leveraging Kafka, Spark, and Hadoop to analyze booking and clickstream data. Implemented data models that improved data throughput by 15% and provided critical insights for decision-making, enhancing operational efficiency.'},\n",
       "  {'proj_name': 'Spar Nord Bank ETL',\n",
       "   'proj_start_date': '2024-12-04',\n",
       "   'proj_end_date': '2024-12-12',\n",
       "   'proj_description': 'Managed the ETL process for transactional data from MySQL RDS to Amazon Redshift. Conducted thorough data analysis to identify patterns in usage, optimizing ATM refueling processes and achieving a 20% reduction in unnecessary refills.'}],\n",
       " 'education': [{'edu_course': 'Master of Science in Data Analytics',\n",
       "   'edu_institution': 'San Jose State University',\n",
       "   'edu_start_date': '2024-12-02',\n",
       "   'edu_end_date': '2024-12-11'},\n",
       "  {'edu_course': 'PG Diploma in Data Science Specialization in Data Engineering',\n",
       "   'edu_institution': 'Visvesvaraya Technological University',\n",
       "   'edu_start_date': '2024-12-03',\n",
       "   'edu_end_date': '2024-12-04'},\n",
       "  {'edu_course': 'Bachelor’s in Computer Science',\n",
       "   'edu_institution': 'Indian Institute of Information Technology',\n",
       "   'edu_start_date': '2024-12-11',\n",
       "   'edu_end_date': '2024-12-12'}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Pt, Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_BREAK\n",
    "from docx.enum.table import WD_ALIGN_VERTICAL\n",
    "from docx.oxml import parse_xml\n",
    "from docx.oxml.ns import nsdecls\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Section:\n",
    "    title: str\n",
    "    content_generator: callable\n",
    "\n",
    "class ResumeGenerator:\n",
    "    MARGIN = 0.5\n",
    "    DATE_INPUT_FORMAT = '%Y-%m-%d'\n",
    "    DATE_OUTPUT_FORMAT = '%b %Y'\n",
    "    NAME_FONT_SIZE = 14\n",
    "    DEFAULT_FONT_SIZE = 11\n",
    "\n",
    "    def __init__(self, template_path: str):\n",
    "        self.template = Document(template_path)\n",
    "        self.document = Document()\n",
    "        self._setup_margins()\n",
    "\n",
    "    def _setup_margins(self) -> None:\n",
    "        for section in self.document.sections:\n",
    "            for margin in ['top_margin', 'bottom_margin', 'left_margin', 'right_margin']:\n",
    "                setattr(section, margin, Inches(self.MARGIN))\n",
    "\n",
    "    def _format_date(self, date_str: str) -> str:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, self.DATE_INPUT_FORMAT)\n",
    "            return date_obj.strftime(self.DATE_OUTPUT_FORMAT).capitalize()\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Invalid date format. Expected YYYY-MM-DD, got {date_str}\") from e\n",
    "\n",
    "    def _add_bottom_border(self, paragraph) -> None:\n",
    "        p = paragraph._p\n",
    "        pPr = p.get_or_add_pPr()\n",
    "        bottom = parse_xml(f'<w:pBdr {nsdecls(\"w\")}><w:bottom w:val=\"single\" w:sz=\"4\" w:space=\"1\" w:color=\"auto\"/></w:pBdr>')\n",
    "        pPr.append(bottom)\n",
    "\n",
    "    def _add_paragraph(self, text: str = \"\", bold: bool = False, spacing: int = 1, \n",
    "                      font_size: int = DEFAULT_FONT_SIZE, align_center: bool = False,\n",
    "                      add_border: bool = False) -> None:\n",
    "        paragraph = self.document.add_paragraph()\n",
    "        if text:\n",
    "            run = paragraph.add_run(text)\n",
    "            run.bold = bold\n",
    "            font = run.font\n",
    "            font.size = Pt(font_size)\n",
    "            font.name = \"Times New Roman\" \n",
    "        if align_center:\n",
    "            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        paragraph.paragraph_format.space_before = Pt(spacing)\n",
    "        paragraph.paragraph_format.space_after = Pt(spacing)\n",
    "        if add_border:\n",
    "            self._add_bottom_border(paragraph)\n",
    "        return paragraph\n",
    "\n",
    "    def _add_two_column_table(self, left_text: str, right_text: str, bold: bool = False) -> None:\n",
    "        table = self.document.add_table(rows=1, cols=2)\n",
    "        table.autofit = True\n",
    "        table.allow_autofit = True\n",
    "        left_cell = table.cell(0, 0)\n",
    "        left_paragraph = left_cell.paragraphs[0]\n",
    "        left_run = left_paragraph.add_run(left_text)\n",
    "        left_run.font.size = Pt(self.DEFAULT_FONT_SIZE)\n",
    "        left_run.font.name = \"Times New Roman\"\n",
    "        left_paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "        left_paragraph.paragraph_format.space_before = Pt(0)\n",
    "        left_paragraph.paragraph_format.space_after = Pt(0)\n",
    "\n",
    "        right_cell = table.cell(0, 1)\n",
    "        right_paragraph = right_cell.paragraphs[0]\n",
    "        right_run = right_paragraph.add_run(right_text)\n",
    "        right_run.font.size = Pt(self.DEFAULT_FONT_SIZE)\n",
    "        right_run.font.name = \"Times New Roman\"\n",
    "        right_paragraph.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "        right_paragraph.paragraph_format.space_before = Pt(0)\n",
    "        right_paragraph.paragraph_format.space_after = Pt(0)\n",
    "\n",
    "    def _add_header_section(self, data: Dict[str, Any]) -> None:\n",
    "        self._add_paragraph(data['user_name'], bold=True, font_size=self.NAME_FONT_SIZE, align_center=True)\n",
    "        contact_info = f\"{data['user_city']} | {data['user_phone']} | {data['user_email']}\"\n",
    "        if data['user_linkedin_link']:\n",
    "            contact_info += f\" | {data['user_linkedin_link']}\"\n",
    "        self._add_paragraph(contact_info, align_center=True)\n",
    "        self._add_paragraph(\"Summary\", bold=True, add_border=True)\n",
    "        self._add_paragraph(data['user_summary'])\n",
    "\n",
    "    def _add_experience_section(self, experiences: List[Dict[str, Any]]) -> None:\n",
    "        self._add_paragraph(\"Work Experience\", bold=True, add_border=True)\n",
    "        for exp in experiences:\n",
    "            left_text = f\"{exp['exp_role']} | {exp['exp_company']}\"\n",
    "            right_text = f\"{self._format_date(exp['exp_start_date'])} - {self._format_date(exp['exp_end_date'])}\"\n",
    "            self._add_two_column_table(left_text, right_text, bold=True)\n",
    "            description_points = exp['exp_description'].split('.')\n",
    "            for point in description_points:\n",
    "                if point.strip():\n",
    "                    self._add_paragraph(f\"• {point.strip()}\", spacing=1)\n",
    "\n",
    "    def _add_skills_section(self, skills: str) -> None:\n",
    "        self._add_paragraph(\"Skills\", bold=True, add_border=True)\n",
    "        for skill in skills.split('\\n'):\n",
    "            if skill.strip():\n",
    "                self._add_paragraph(f\"{skill.strip()}.\", spacing=1)\n",
    "\n",
    "    def _add_projects_section(self, projects: List[Dict[str, Any]]) -> None:\n",
    "        self._add_paragraph(\"Academic Projects\", bold=True, add_border=True)\n",
    "        for project in projects:\n",
    "            self._add_two_column_table(\n",
    "                project['proj_name'],\n",
    "                f\"{self._format_date(project['proj_start_date'])} - {self._format_date(project['proj_end_date'])}\",\n",
    "                bold=True\n",
    "            )\n",
    "            description_points = project['proj_description'].split('.')\n",
    "            for point in description_points:\n",
    "                if point.strip():\n",
    "                    self._add_paragraph(f\"• {point.strip()}\", spacing=1)\n",
    "\n",
    "    def _add_education_section(self, education: List[Dict[str, Any]]) -> None:\n",
    "        self._add_paragraph(\"Education\", bold=True, add_border=True)\n",
    "        for edu in education:\n",
    "            edu_institution_abbr = \"\".join([word[0] for word in edu['edu_institution'].split(\" \") if not word[0].islower()])\n",
    "            self._add_two_column_table(\n",
    "                f\"{edu['edu_course']} | {edu_institution_abbr}\",\n",
    "                f\"{self._format_date(edu['edu_start_date'])} - {self._format_date(edu['edu_end_date'])}\",\n",
    "                bold=True\n",
    "            )\n",
    "\n",
    "    def generate(self, data: Dict[str, Any], output_path: str) -> None:\n",
    "        # Set the default font for the document\n",
    "        style = self.document.styles['Normal']\n",
    "        font = style.font\n",
    "        font.name = \"Times New Roman\"  # Set default font to Times New Roman\n",
    "        font.size = Pt(self.DEFAULT_FONT_SIZE)\n",
    "        sections = [\n",
    "            Section(\"Header\", lambda: self._add_header_section(data)),\n",
    "            Section(\"Experience\", lambda: self._add_experience_section(data['experiences'])),\n",
    "            Section(\"Skills\", lambda: self._add_skills_section(data['user_skills'])),\n",
    "            Section(\"Projects\", lambda: self._add_projects_section(data['projects'])),\n",
    "            Section(\"Education\", lambda: self._add_education_section(data['education']))\n",
    "        ]\n",
    "        for section in sections:\n",
    "            try:\n",
    "                section.content_generator()\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error generating {section.title} section: {str(e)}\")\n",
    "        style = self.document.styles['Normal']\n",
    "        style.paragraph_format.line_spacing = 1\n",
    "        self.document.save(output_path)\n",
    "\n",
    "def fill_resume(sourcepath: str, destpath: str, data: Dict[str, Any]) -> None:\n",
    "    try:\n",
    "        generator = ResumeGenerator(sourcepath)\n",
    "        generator.generate(data, destpath)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to generate resume: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/Users/reddy/Documents/GitHub/FastApply/FastApply/inputs/template.docx\"\n",
    "dest = \"/Users/reddy/Documents/GitHub/FastApply/FastApply/outputs/test.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_resume(source, dest, resume_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certificates:\n",
      "AWS\n",
      "Cloud\n",
      "Practitioner\n",
      "|\n",
      "Validation\n",
      "Code:\n",
      "QD8E074C82EQ1WCE\n",
      "Programming\n",
      "Languages:\n",
      "Python,\n",
      "Java,\n",
      "C++,\n",
      "SQL,\n",
      "R,\n",
      "Bash\n",
      "Cloud\n",
      "Services:\n",
      "AWS\n",
      "(EC2,\n",
      "S3,\n",
      "Lambda,\n",
      "Glue,\n",
      "Amazon\n",
      "RDS,\n",
      "Amazon\n",
      "Redshift),\n",
      "GCP,\n",
      "Azure\n",
      "Databases:\n",
      "MySQL,\n",
      "PostgreSQL,\n",
      "MongoDB,\n",
      "RDS,\n",
      "Elasticsearch.\n",
      "Tools:\n",
      "Hadoop,\n",
      "Spark,\n",
      "PySpark,\n",
      "Airflow,\n",
      "Kafka,\n",
      "Pandas,\n",
      "Scikit-Learn,\n",
      "Keras,\n",
      "Tableau,\n",
      "Docker,\n",
      "Terraform..\n"
     ]
    }
   ],
   "source": [
    "input_string = \"Certificates: AWS Cloud Practitioner | Validation Code: QD8E074C82EQ1WCE Programming Languages: Python, Java, C++, SQL, R, Bash Cloud Services: AWS (EC2, S3, Lambda, Glue, Amazon RDS, Amazon Redshift), GCP, Azure Databases: MySQL, PostgreSQL, MongoDB, RDS, Elasticsearch. Tools: Hadoop, Spark, PySpark, Airflow, Kafka, Pandas, Scikit-Learn, Keras, Tableau, Docker, Terraform..\"\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "# Split the input string into lines by looking for key-value pairs separated by a colon\n",
    "items = input_string.split(' ')\n",
    "for item in items:\n",
    "    print(item)\n",
    "    # if ': ' in item:\n",
    "    #     key, value = item.split(': ', 1)\n",
    "    #     data_dict[key.strip()] = value.strip()\n",
    "\n",
    "# print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_resume_dict = { \"user_name\": \"Shashank Shashishekhar Reddy\", \"user_email\": \"shashankshashishekharreddy@gmail.com\", \"user_phone\": \"(510) 892-7191\", \"user_city\": \"San Jose, California\", \"user_linkedin_link\": \"https://www.linkedin.com/in/snkreddy\", \"user_summary\": \"Data Engineer with extensive experience in designing, deploying, and managing data applications and infrastructures to support high-throughput manufacturing environments. Proficient in developing complex data retrieval solutions and conducting statistical analyses to drive data-driven insights for production optimization. Highly skilled in collaborating with cross-functional teams to adapt data solutions to evolving business needs, ensuring scalability and efficiency in manufacturing processes.\", \"user_skills\": \"Certificates: AWS Cloud Practitioner | Validation Code: QD8E074C82EQ1WCE Programming Languages: Python, Java, C, C++, R, SQL, Bash, JavaScript Cloud Services: AWS (EC2, S3, Lambda, Glue), GCP, Azure Databases: MySQL, MongoDB, PostgreSQL, RDS, Elasticsearch. Tools: Hadoop, Spark, Sqoop, Hive, PySpark, Kafka, Docker, Terraform, JMP.\", \"experiences\": [ { \"exp_company\": \"Kantar\", \"exp_role\": \"Data Engineer\", \"exp_start_date\": \"2024-12-03\", \"exp_end_date\": \"2024-12-04\", \"exp_description\": \"Administered and enhanced data applications using Apache Spark and Airflow to monitor and optimize production processes for ads and sales data. Developed models for key statistics, ensuring sound documentation for functionalities and procedures. Facilitated training sessions for end-users, improving understanding of data applications which resulted in better data utilization across teams.\" }, { \"exp_company\": \"The Sparks Foundation\", \"exp_role\": \"Data Engineer\", \"exp_start_date\": \"2024-12-11\", \"exp_end_date\": \"2024-12-12\", \"exp_description\": \"Engineered and maintained ETL pipelines to consolidate various data sources, yielding a 25% improvement in data integration. Collaborated closely with engineering teams to provide data analysis supporting production processes, achieving a 30% reduction in retrieval times through optimized querying and indexing strategies. Generated dashboards for stakeholder insights through real-time data visualization.\" }, { \"exp_company\": \"The Sparks Foundation\", \"exp_role\": \"Data Engineer Intern\", \"exp_start_date\": \"2024-12-04\", \"exp_end_date\": \"2024-12-12\", \"exp_description\": \"Supported the administration of data applications and contributed to the seamless integration of new data sources. Assisted in monitoring production data analytics to inform engineering decisions. Enhanced data processing efficiency by automating data cleaning processes, thus improving the accuracy of data insights.\" } ], \"projects\": [ { \"proj_name\": \"Your Own Cabs Analysis\", \"proj_start_date\": \"2024-12-04\", \"proj_end_date\": \"2024-12-05\", \"proj_description\": \"Designed and implemented data pipelines for booking and clickstream analysis using Kafka, Spark, and Hadoop. Enhanced data ingestion processes, processing clickstream data efficiently into AWS components, and generating actionable insights to optimize customer engagement and service quality.\" }, { \"proj_name\": \"Spar Nord Bank ETL\", \"proj_start_date\": \"2024-12-04\", \"proj_end_date\": \"2024-12-12\", \"proj_description\": \"Managed ETL processes for Spar Nord Bank’s transactional data, delivering optimized data flow into Amazon Redshift. Conducted comprehensive analyses to highlight usage patterns, influencing operational strategies for ATM refilling and enhancing overall efficiency.\" } ], \"education\": [ { \"edu_course\": \"Master of Science in Data Analytics\", \"edu_institution\": \"SJSU\", \"edu_start_date\": \"2024-12-02\", \"edu_end_date\": \"2024-12-11\" }, { \"edu_course\": \"PG Diploma in Data Science Specialization in Data Engineering\", \"edu_institution\": \"VTU\", \"edu_start_date\": \"2024-12-03\", \"edu_end_date\": \"2024-12-04\" }, { \"edu_course\": \"Bachelor’s in Computer Science\", \"edu_institution\": \"IIIT\", \"edu_start_date\": \"2024-12-11\", \"edu_end_date\": \"2024-12-12\" } ] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Shashank Shashishekhar Reddy',\n",
       " 'user_email': 'shashankshashishekharreddy@gmail.com',\n",
       " 'user_phone': '(510) 892-7191',\n",
       " 'user_city': 'San Jose, California',\n",
       " 'user_linkedin_link': 'https://www.linkedin.com/in/snkreddy',\n",
       " 'user_summary': 'Data Engineer with extensive experience in designing, deploying, and managing data applications and infrastructures to support high-throughput manufacturing environments. Proficient in developing complex data retrieval solutions and conducting statistical analyses to drive data-driven insights for production optimization. Highly skilled in collaborating with cross-functional teams to adapt data solutions to evolving business needs, ensuring scalability and efficiency in manufacturing processes.',\n",
       " 'user_skills': 'Certificates: AWS Cloud Practitioner | Validation Code: QD8E074C82EQ1WCE Programming Languages: Python, Java, C, C++, R, SQL, Bash, JavaScript Cloud Services: AWS (EC2, S3, Lambda, Glue), GCP, Azure Databases: MySQL, MongoDB, PostgreSQL, RDS, Elasticsearch. Tools: Hadoop, Spark, Sqoop, Hive, PySpark, Kafka, Docker, Terraform, JMP.',\n",
       " 'experiences': [{'exp_company': 'Kantar',\n",
       "   'exp_role': 'Data Engineer',\n",
       "   'exp_start_date': '2024-12-03',\n",
       "   'exp_end_date': '2024-12-04',\n",
       "   'exp_description': 'Administered and enhanced data applications using Apache Spark and Airflow to monitor and optimize production processes for ads and sales data. Developed models for key statistics, ensuring sound documentation for functionalities and procedures. Facilitated training sessions for end-users, improving understanding of data applications which resulted in better data utilization across teams.'},\n",
       "  {'exp_company': 'The Sparks Foundation',\n",
       "   'exp_role': 'Data Engineer',\n",
       "   'exp_start_date': '2024-12-11',\n",
       "   'exp_end_date': '2024-12-12',\n",
       "   'exp_description': 'Engineered and maintained ETL pipelines to consolidate various data sources, yielding a 25% improvement in data integration. Collaborated closely with engineering teams to provide data analysis supporting production processes, achieving a 30% reduction in retrieval times through optimized querying and indexing strategies. Generated dashboards for stakeholder insights through real-time data visualization.'},\n",
       "  {'exp_company': 'The Sparks Foundation',\n",
       "   'exp_role': 'Data Engineer Intern',\n",
       "   'exp_start_date': '2024-12-04',\n",
       "   'exp_end_date': '2024-12-12',\n",
       "   'exp_description': 'Supported the administration of data applications and contributed to the seamless integration of new data sources. Assisted in monitoring production data analytics to inform engineering decisions. Enhanced data processing efficiency by automating data cleaning processes, thus improving the accuracy of data insights.'}],\n",
       " 'projects': [{'proj_name': 'Your Own Cabs Analysis',\n",
       "   'proj_start_date': '2024-12-04',\n",
       "   'proj_end_date': '2024-12-05',\n",
       "   'proj_description': 'Designed and implemented data pipelines for booking and clickstream analysis using Kafka, Spark, and Hadoop. Enhanced data ingestion processes, processing clickstream data efficiently into AWS components, and generating actionable insights to optimize customer engagement and service quality.'},\n",
       "  {'proj_name': 'Spar Nord Bank ETL',\n",
       "   'proj_start_date': '2024-12-04',\n",
       "   'proj_end_date': '2024-12-12',\n",
       "   'proj_description': 'Managed ETL processes for Spar Nord Bank’s transactional data, delivering optimized data flow into Amazon Redshift. Conducted comprehensive analyses to highlight usage patterns, influencing operational strategies for ATM refilling and enhancing overall efficiency.'}],\n",
       " 'education': [{'edu_course': 'Master of Science in Data Analytics',\n",
       "   'edu_institution': 'SJSU',\n",
       "   'edu_start_date': '2024-12-02',\n",
       "   'edu_end_date': '2024-12-11'},\n",
       "  {'edu_course': 'PG Diploma in Data Science Specialization in Data Engineering',\n",
       "   'edu_institution': 'VTU',\n",
       "   'edu_start_date': '2024-12-03',\n",
       "   'edu_end_date': '2024-12-04'},\n",
       "  {'edu_course': 'Bachelor’s in Computer Science',\n",
       "   'edu_institution': 'IIIT',\n",
       "   'edu_start_date': '2024-12-11',\n",
       "   'edu_end_date': '2024-12-12'}]}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_resume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
